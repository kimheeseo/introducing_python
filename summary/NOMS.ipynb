{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7g10ffZwSqXNHobiQm31n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimheeseo/python_summary/blob/main/NOMS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Tol0WyWT6Sq",
        "outputId": "55aeef0e-2d92-4470-a860-dc0f17884d23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **출처 : https://github.com/lorenlugosch/neural-min-sum-decoding**"
      ],
      "metadata": {
        "id": "U2EHxibvT7mo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViS3CBFgxCyg",
        "outputId": "358b51c3-7561-4ca1-f537-73ed65a465c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My PID: 387\n",
            "Using Min-Sum algorithm\n",
            "Training using only the all-zeros codeword\n",
            "Testing using random codewords (not the all-zeros codeword)\n",
            "Scaling train input by 2/sigma\n",
            "Scaling test input by 2/sigma\n",
            "\n",
            "\n",
            "Decoder type: FNOMS\n",
            "\n",
            "\n",
            "not relaxed\n",
            "\n",
            "Building the decoder graph...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-6aa322f6c99d>:129: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  new_order = np.zeros(num_edges).astype(np.int)\n",
            "<ipython-input-4-6aa322f6c99d>:130: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  new_order[edge_order] = np.array(range(0,num_edges)).astype(np.int)\n",
            "<ipython-input-4-6aa322f6c99d>:179: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  new_order = np.zeros(num_edges).astype(np.int)\n",
            "<ipython-input-4-6aa322f6c99d>:180: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  new_order[edge_order] = np.array(range(0,num_edges)).astype(np.int)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L = 0.5\n",
            "Learning rate: 0.01\n",
            "Done.\n",
            "\n",
            "***********************\n",
            "Training decoder using 100 minibatches...\n",
            "***********************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-6aa322f6c99d>:368: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  sigma = np.sqrt(1. / (2 * (np.float(k)/np.float(n)) * 10**(SNRs[i]/10)))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 minibatches completed\n",
            "Trained decoder on 100 minibatches.\n",
            "\n",
            "***********************\n",
            "Testing decoder...\n",
            "***********************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-6aa322f6c99d>:401: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  sigma = np.sqrt(1. / (2 * (np.float(k)/np.float(n)) * 10**(SNR/10)))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SNR: 1\n",
            "frame count: 100080\n",
            "bit errors: 42671\n",
            "BER: 0.06090984355372844\n",
            "FER: 0.1924660271782574\n",
            "\n",
            "SNR: 2\n",
            "frame count: 100080\n",
            "bit errors: 24503\n",
            "BER: 0.034976304670549276\n",
            "FER: 0.11336930455635491\n",
            "\n",
            "SNR: 3\n",
            "frame count: 100080\n",
            "bit errors: 12325\n",
            "BER: 0.01759306840242092\n",
            "FER: 0.05748401278976818\n",
            "\n",
            "SNR: 4\n",
            "frame count: 100080\n",
            "bit errors: 5160\n",
            "BER: 0.00736553614251456\n",
            "FER: 0.024500399680255795\n",
            "\n",
            "SNR: 5\n",
            "frame count: 100080\n",
            "bit errors: 1612\n",
            "BER: 0.002301016329793308\n",
            "FER: 0.007753796962430056\n",
            "\n",
            "SNR: 6\n",
            "frame count: 100080\n",
            "bit errors: 430\n",
            "BER: 0.00061379467854288\n",
            "FER: 0.002138289368505196\n",
            "\n",
            "BERs:\n",
            "[0.06090984355372844, 0.034976304670549276, 0.01759306840242092, 0.00736553614251456, 0.002301016329793308, 0.00061379467854288]\n",
            "FERs:\n",
            "[0.1924660271782574, 0.11336930455635491, 0.05748401278976818, 0.024500399680255795, 0.007753796962430056, 0.002138289368505196]\n"
          ]
        }
      ],
      "source": [
        "# Belief propagation using TensorFlow\n",
        "# Run as follows:\n",
        "# python main.py 0 1 6 1 100 10000000000000000 5 hamming.alist hamming.gmat laskdjhf 0.5 100 FNOMS\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "from tensorflow.python.framework import ops\n",
        "from helper_functions import load_code, syndrome\n",
        "import os\n",
        "\n",
        "\n",
        "DEBUG = False\n",
        "TRAINING = True\n",
        "SUM_PRODUCT = False\n",
        "MIN_SUM = not SUM_PRODUCT\n",
        "ALL_ZEROS_CODEWORD_TRAINING = True \n",
        "ALL_ZEROS_CODEWORD_TESTING = False\n",
        "NO_SIGMA_SCALING_TRAIN = False\n",
        "NO_SIGMA_SCALING_TEST = False\n",
        "np.set_printoptions(precision=3)\n",
        "\n",
        "print(\"My PID: \" + str(os.getpid()))\n",
        "\n",
        "if SUM_PRODUCT:\n",
        "   print(\"Using Sum-Product algorithm\")\n",
        "if MIN_SUM:\n",
        "   print(\"Using Min-Sum algorithm\")\n",
        "\n",
        "if ALL_ZEROS_CODEWORD_TRAINING:\n",
        "   print(\"Training using only the all-zeros codeword\")\n",
        "else:\n",
        "   print(\"Training using random codewords (not the all-zeros codeword)\")\n",
        "\n",
        "if ALL_ZEROS_CODEWORD_TESTING:\n",
        "   print(\"Testing using only the all-zeros codeword\")\n",
        "else:\n",
        "   print(\"Testing using random codewords (not the all-zeros codeword)\")\n",
        "\n",
        "if NO_SIGMA_SCALING_TRAIN:\n",
        "   print(\"Not scaling train input by 2/sigma\")\n",
        "else:\n",
        "   print(\"Scaling train input by 2/sigma\")\n",
        "\n",
        "if NO_SIGMA_SCALING_TEST:\n",
        "   print(\"Not scaling test input by 2/sigma\")\n",
        "else:\n",
        "   print(\"Scaling test input by 2/sigma\")\n",
        "\n",
        "\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "seed = 0#int(sys.argv[1])\n",
        "np.random.seed(seed)\n",
        "snr_lo = 1#float(sys.argv[2])\n",
        "snr_hi = 6#float(sys.argv[3])\n",
        "snr_step = 1#float(sys.argv[4])\n",
        "min_frame_errors = 100#int(sys.argv[5])\n",
        "max_frames = 10000000000000000#float(sys.argv[6])\n",
        "num_iterations = 5#int(sys.argv[7])\n",
        "H_filename = \"hamming.alist\"#sys.argv[8]\n",
        "G_filename = \"hamming.gmat\"#sys.argv[9]\n",
        "output_filename = \"laskdjhf\"#sys.argv[10]\n",
        "L = 0.5#float(sys.argv[11])\n",
        "steps = 100#int(sys.argv[12])\n",
        "provided_decoder_type = \"FNOMS\"#sys.argv[13]\n",
        "\n",
        "if ALL_ZEROS_CODEWORD_TESTING: G_filename = \"\"\n",
        "code = load_code(H_filename, G_filename)\n",
        "\n",
        "# code.H = np.array([[1, 1, 0, 1, 1, 0, 0],\n",
        "#        [1, 0, 1, 1, 0, 1, 0],\n",
        "#        [0, 1, 1, 1, 0, 0, 1]])\n",
        "\n",
        "H = code.H\n",
        "G = code.G\n",
        "var_degrees = code.var_degrees\n",
        "chk_degrees = code.chk_degrees\n",
        "num_edges = code.num_edges\n",
        "u = code.u\n",
        "d = code.d\n",
        "n = code.n\n",
        "m = code.m\n",
        "k = code.k\n",
        "\n",
        "class Decoder:\n",
        "   def __init__(self, decoder_type=\"RNOMS\", random_seed=0, learning_rate = 0.001, relaxed = False):\n",
        "      self.decoder_type = decoder_type\n",
        "      self.random_seed = random_seed\n",
        "      self.learning_rate = learning_rate\n",
        "      self.relaxed = relaxed\n",
        "\n",
        "# decoder parameters\n",
        "batch_size = 120\n",
        "tf_train_dataset = tf.compat.v1.placeholder(tf.float32, shape=(n,batch_size))\n",
        "tf_train_labels = tf.compat.v1.placeholder(tf.float32, shape=(n,batch_size))#tf.placeholder(tf.float32, shape=(num_iterations,n,batch_size))\n",
        "\n",
        "#### decoder functions ####\n",
        "\n",
        "# compute messages from variable nodes to check nodes\n",
        "def compute_vc(cv, iteration, soft_input):\n",
        "   weighted_soft_input = soft_input\n",
        "   \n",
        "   edges = []\n",
        "   for i in range(0, n):\n",
        "      for j in range(0, var_degrees[i]):\n",
        "         edges.append(i)\n",
        "   reordered_soft_input = tf.gather(weighted_soft_input, edges)\n",
        "   \n",
        "   vc = []\n",
        "   edge_order = []\n",
        "   for i in range(0, n): # for each variable node v\n",
        "      for j in range(0, var_degrees[i]):\n",
        "         # edge = d[i][j]\n",
        "         edge_order.append(d[i][j])\n",
        "         extrinsic_edges = []\n",
        "         for jj in range(0, var_degrees[i]):\n",
        "            if jj != j: # extrinsic information only\n",
        "               extrinsic_edges.append(d[i][jj])\n",
        "         # if the list of edges is not empty, add them up\n",
        "         if extrinsic_edges:\n",
        "            temp = tf.gather(cv,extrinsic_edges)\n",
        "            temp = tf.reduce_sum(input_tensor=temp,axis=0)\n",
        "         else:\n",
        "            temp = tf.zeros([batch_size])\n",
        "         if SUM_PRODUCT: temp = tf.cast(temp, tf.float32)#tf.cast(temp, tf.float64)\n",
        "         vc.append(temp)\n",
        "   \n",
        "   vc = tf.stack(vc)\n",
        "   new_order = np.zeros(num_edges).astype(np.int)\n",
        "   new_order[edge_order] = np.array(range(0,num_edges)).astype(np.int)\n",
        "   vc = tf.gather(vc,new_order)\n",
        "   vc = vc + reordered_soft_input\n",
        "   return vc\n",
        "\n",
        "# compute messages from check nodes to variable nodes\n",
        "def compute_cv(vc, iteration):\n",
        "   cv_list = []\n",
        "   prod_list = []\n",
        "   min_list = []\n",
        "   \n",
        "   if SUM_PRODUCT:\n",
        "      vc = tf.clip_by_value(vc, -10, 10)\n",
        "      tanh_vc = tf.tanh(vc / 2.0)\n",
        "   edge_order = []\n",
        "   for i in range(0, m): # for each check node c\n",
        "      for j in range(0, chk_degrees[i]):\n",
        "         # edge = u[i][j]\n",
        "         edge_order.append(u[i][j])\n",
        "         extrinsic_edges = []\n",
        "         for jj in range(0, chk_degrees[i]):\n",
        "            if jj != j:\n",
        "               extrinsic_edges.append(u[i][jj])\n",
        "         if SUM_PRODUCT:\n",
        "            temp = tf.gather(tanh_vc,extrinsic_edges)\n",
        "            temp = tf.reduce_prod(input_tensor=temp,axis=0)\n",
        "            temp = tf.math.log((1+temp)/(1-temp))\n",
        "            cv_list.append(temp)\n",
        "         if MIN_SUM:\n",
        "            temp = tf.gather(vc,extrinsic_edges)\n",
        "            temp1 = tf.reduce_prod(input_tensor=tf.sign(temp),axis=0)\n",
        "            temp2 = tf.reduce_min(input_tensor=tf.abs(temp),axis=0)\n",
        "            prod_list.append(temp1)\n",
        "            min_list.append(temp2)\n",
        "   \n",
        "   if SUM_PRODUCT:\n",
        "      cv = tf.stack(cv_list)\n",
        "   if MIN_SUM:\n",
        "      prods = tf.stack(prod_list)\n",
        "      mins = tf.stack(min_list)\n",
        "      if decoder.decoder_type == \"RNOMS\":\n",
        "         # offsets = tf.nn.softplus(decoder.B_cv)\n",
        "         # mins = tf.nn.relu(mins - tf.tile(tf.reshape(offsets,[-1,1]),[1,batch_size]))\n",
        "         mins = tf.nn.relu(mins - decoder.B_cv)\n",
        "      elif decoder.decoder_type == \"FNOMS\":\n",
        "         offsets = tf.nn.softplus(decoder.B_cv[iteration])\n",
        "         mins = tf.nn.relu(mins - tf.tile(tf.reshape(offsets,[-1,1]),[1,batch_size]))\n",
        "      cv = prods * mins\n",
        "   \n",
        "   new_order = np.zeros(num_edges).astype(np.int)\n",
        "   new_order[edge_order] = np.array(range(0,num_edges)).astype(np.int)\n",
        "   cv = tf.gather(cv,new_order)\n",
        "   \n",
        "   if decoder.decoder_type == \"RNSPA\" or decoder.decoder_type == \"RNNMS\":\n",
        "      cv = cv * tf.tile(tf.reshape(decoder.W_cv,[-1,1]),[1,batch_size])\n",
        "   elif decoder.decoder_type == \"FNSPA\" or decoder.decoder_type == \"FNNMS\":\n",
        "      cv = cv * tf.tile(tf.reshape(decoder.W_cv[iteration],[-1,1]),[1,batch_size])\n",
        "   return cv\n",
        "\n",
        "# combine messages to get posterior LLRs\n",
        "def marginalize(soft_input, iteration, cv):\n",
        "   weighted_soft_input = soft_input\n",
        "\n",
        "   soft_output = []\n",
        "   for i in range(0,n):\n",
        "      edges = []\n",
        "      for e in range(0,var_degrees[i]):\n",
        "         edges.append(d[i][e])\n",
        "\n",
        "      temp = tf.gather(cv,edges)\n",
        "      temp = tf.reduce_sum(input_tensor=temp,axis=0)\n",
        "      soft_output.append(temp)\n",
        "\n",
        "   soft_output = tf.stack(soft_output)\n",
        "\n",
        "   soft_output = weighted_soft_input + soft_output\n",
        "   return soft_output\n",
        "\n",
        "def continue_condition(soft_input, soft_output, iteration, cv, m_t, loss, labels):\n",
        "   condition = (iteration < num_iterations)\n",
        "   return condition\n",
        "\n",
        "def belief_propagation_iteration(soft_input, soft_output, iteration, cv, m_t, loss, labels):\n",
        "   # compute vc\n",
        "   vc = compute_vc(cv,iteration,soft_input)\n",
        "\n",
        "   # filter vc\n",
        "   if decoder.relaxed:\n",
        "      m_t = R * m_t + (1-R) * vc\n",
        "      vc_prime = m_t\n",
        "   else:\n",
        "      vc_prime = vc\n",
        "\n",
        "   # compute cv\n",
        "   cv = compute_cv(vc_prime,iteration)\n",
        "\n",
        "   # get output for this iteration\n",
        "   soft_output = marginalize(soft_input, iteration, cv)\n",
        "   iteration += 1\n",
        "\n",
        "   # L = 0.5\n",
        "   print(\"L = \" + str(L))\n",
        "   CE_loss = tf.reduce_mean(input_tensor=tf.nn.sigmoid_cross_entropy_with_logits(logits=-soft_output, labels=labels)) / num_iterations\n",
        "   syndrome_loss = tf.reduce_mean(input_tensor=tf.maximum(1. - syndrome(soft_output, code),0) ) / num_iterations\n",
        "   new_loss = L * CE_loss + (1-L) * syndrome_loss\n",
        "   loss = loss + new_loss\n",
        "\n",
        "   return soft_input, soft_output, iteration, cv, m_t, loss, labels\n",
        "\n",
        "# builds a belief propagation TF graph\n",
        "def belief_propagation_op(soft_input, labels):\n",
        "    return tf.while_loop(\n",
        "      cond=continue_condition, # iteration < max iteration?\n",
        "      body=belief_propagation_iteration, # compute messages for this iteration\n",
        "      loop_vars=[\n",
        "         soft_input, # soft input for this iteration\n",
        "         soft_input,  # soft output for this iteration\n",
        "         tf.constant(0,dtype=tf.int32), # iteration number\n",
        "         tf.zeros([num_edges,batch_size],dtype=tf.float32), # cv\n",
        "         tf.zeros([num_edges,batch_size],dtype=tf.float32), # m_t\n",
        "         tf.constant(0.0,dtype=tf.float32), # loss\n",
        "         labels\n",
        "      ]\n",
        "      )\n",
        "\n",
        "#### end decoder functions ####\n",
        "global_step = tf.Variable(0, trainable=False)\n",
        "starter_learning_rate = 0.01\n",
        "learning_rate = starter_learning_rate # provided_decoder_type=\"normal\", \"FNNMS\", \"FNOMS\", ...\n",
        "decoder = Decoder(decoder_type=provided_decoder_type, random_seed=1, learning_rate = learning_rate, relaxed = False)\n",
        "print(\"\\n\\nDecoder type: \" + decoder.decoder_type + \"\\n\\n\")\n",
        "if decoder.relaxed: print(\"relaxed\")\n",
        "else: print(\"not relaxed\")\n",
        "\n",
        "if SUM_PRODUCT:\n",
        "   if decoder.decoder_type == \"FNSPA\":\n",
        "      decoder.W_cv = tf.Variable(tf.random.truncated_normal([num_iterations, num_edges],dtype=tf.float32,stddev=1.0, seed=decoder.random_seed))\n",
        "      \n",
        "   if decoder.decoder_type == \"RNSPA\":\n",
        "      decoder.W_cv = tf.Variable(tf.random.truncated_normal([num_edges],dtype=tf.float32,stddev=1.0, seed=decoder.random_seed))#tf.Variable(0.0,dtype=tf.float32)#\n",
        "      \n",
        "if MIN_SUM:\n",
        "   if decoder.decoder_type == \"FNNMS\":\n",
        "      # decoder.W_cv = tf.nn.softplus(tf.Variable(tf.truncated_normal([num_iterations, num_edges],dtype=tf.float32,stddev=1.0, seed=decoder.random_seed)))\n",
        "      decoder.W_cv = tf.Variable(tf.random.truncated_normal([num_iterations, num_edges],dtype=tf.float32,stddev=1.0, seed=decoder.random_seed))\n",
        "      \n",
        "   if decoder.decoder_type == \"FNOMS\":\n",
        "      decoder.B_cv = tf.Variable(tf.random.truncated_normal([num_iterations, num_edges],dtype=tf.float32,stddev=1.0))#tf.Variable(1.0 + tf.truncated_normal([num_iterations, num_edges],dtype=tf.float32,stddev=1.0))#tf.Variable(1.0 + tf.truncated_normal([num_iterations, num_edges],dtype=tf.float32,stddev=1.0/num_edges))\n",
        "\n",
        "   if decoder.decoder_type == \"RNNMS\":\n",
        "      decoder.W_cv = tf.nn.softplus(tf.Variable(tf.random.truncated_normal([num_edges],dtype=tf.float32,stddev=1.0, seed=decoder.random_seed)))#tf.Variable(0.0,dtype=tf.float32)#\n",
        "      \n",
        "   if decoder.decoder_type == \"RNOMS\":\n",
        "      decoder.B_cv = tf.Variable(tf.random.truncated_normal([num_edges],dtype=tf.float32,stddev=1.0)) #tf.Variable(0.0,dtype=tf.float32)#\n",
        "\n",
        "if decoder.relaxed:\n",
        "   decoder.relaxation_factors = tf.Variable(0.0,dtype=tf.float32)\n",
        "   R = tf.sigmoid(decoder.relaxation_factors)\n",
        "   # print \"single learned relaxation factor\"\n",
        "\n",
        "   # decoder.relaxation_factors = tf.Variable(tf.truncated_normal([num_edges],dtype=tf.float32,stddev=1.0))\n",
        "   # R = tf.tile(tf.reshape(tf.sigmoid(decoder.relaxation_factors),[-1,1]),[1,batch_size])\n",
        "   # print \"multiple relaxation factors\"\n",
        "\n",
        "#gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=1)\n",
        "\n",
        "\n",
        "\n",
        "with tf.compat.v1.Session() as session: #tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as session:\n",
        "    # simulate each SNR\n",
        "    SNRs = np.arange(snr_lo, snr_hi+snr_step, snr_step)\n",
        "    if (batch_size % len(SNRs)) != 0:\n",
        "        print(\"********************\")\n",
        "        print(\"********************\")\n",
        "        print(\"error: batch size must divide by the number of SNRs to train on\")\n",
        "        print(\"********************\")\n",
        "        print(\"********************\")\n",
        "    BERs = []\n",
        "    SERs = []\n",
        "    FERs = []\n",
        "    \n",
        "    print(\"\\nBuilding the decoder graph...\")\n",
        "    belief_propagation = belief_propagation_op(soft_input=tf_train_dataset, labels=tf_train_labels)\n",
        "    if TRAINING:\n",
        "        training_loss = belief_propagation[5]#tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=belief_propagation[1], labels=tf_train_labels))\n",
        "        loss = training_loss\n",
        "        print(\"Learning rate: \" + str(starter_learning_rate))\n",
        "        optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=decoder.learning_rate).minimize(loss,global_step=global_step)\n",
        "\n",
        "    print(\"Done.\\n\")\n",
        "    init = tf.compat.v1.global_variables_initializer()\n",
        "    \n",
        "    if ALL_ZEROS_CODEWORD_TRAINING:\n",
        "       codewords = np.zeros([n,batch_size])\n",
        "       codewords_repeated = np.zeros([num_iterations,n,batch_size]) # repeat for each iteration (multiloss)\n",
        "       BPSK_codewords = np.ones([n,batch_size])\n",
        "       soft_input = np.zeros_like(BPSK_codewords)\n",
        "       channel_information = np.zeros_like(BPSK_codewords)\n",
        "    \n",
        "    covariance_matrix = np.eye(n)\n",
        "    eta = 0.99\n",
        "    for i in range(0,n):\n",
        "        for j in range(0,n):\n",
        "            covariance_matrix[i,j] = eta**np.abs(i-j)\n",
        "\n",
        "    session.run(init)\n",
        "   \n",
        "    if TRAINING:\n",
        "        # steps = 10001\n",
        "        print(\"***********************\")\n",
        "        print(\"Training decoder using \" + str(steps) + \" minibatches...\")\n",
        "        print(\"***********************\")\n",
        "\n",
        "        step = 0\n",
        "        while step < steps:\n",
        "            # generate random codewords\n",
        "            if not ALL_ZEROS_CODEWORD_TRAINING:\n",
        "                # generate message\n",
        "                messages = np.random.randint(0,2,[k,batch_size])\n",
        "\n",
        "                # encode message\n",
        "                codewords = np.dot(G, messages) % 2\n",
        "                #codewords_repeated = np.tile(x,(num_iterations,1,1)).shape \n",
        "\n",
        "                # modulate codeword\n",
        "                BPSK_codewords = (0.5 - codewords.astype(np.float32)) * 2.0\n",
        "\n",
        "                soft_input = np.zeros_like(BPSK_codewords)\n",
        "                channel_information = np.zeros_like(BPSK_codewords)\n",
        "            else:\n",
        "                codewords = np.zeros([n,batch_size])\n",
        "                #codewords_repeated = np.zeros([num_iterations,n,batch_size]) # repeat for each iteration (multiloss)\n",
        "                BPSK_codewords = np.ones([n,batch_size])\n",
        "                soft_input = np.zeros_like(BPSK_codewords)\n",
        "                channel_information = np.zeros_like(BPSK_codewords)\n",
        "\n",
        "            # create minibatch with codewords from multiple SNRs\n",
        "            for i in range(0,len(SNRs)):\n",
        "                sigma = np.sqrt(1. / (2 * (np.float(k)/np.float(n)) * 10**(SNRs[i]/10)))\n",
        "                noise = sigma * np.random.randn(n,batch_size//len(SNRs))\n",
        "                start_idx = batch_size*i//len(SNRs)\n",
        "                end_idx = batch_size*(i+1)//len(SNRs)\n",
        "                channel_information[:,start_idx:end_idx] = BPSK_codewords[:,start_idx:end_idx] + noise\n",
        "                if NO_SIGMA_SCALING_TRAIN:\n",
        "                    soft_input[:,start_idx:end_idx] = channel_information[:,start_idx:end_idx]\n",
        "                else:\n",
        "                    soft_input[:,start_idx:end_idx] = 2.0*channel_information[:,start_idx:end_idx]/(sigma*sigma)\n",
        "\n",
        "\n",
        "            # feed minibatch into BP and run SGD\n",
        "            batch_data = soft_input\n",
        "            batch_labels = codewords #codewords #codewords_repeated\n",
        "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
        "            [_] = session.run([optimizer], feed_dict=feed_dict) #,bp_output,syndrome_output,belief_propagation, soft_syndromes\n",
        "\n",
        "            if decoder.relaxed and TRAINING: \n",
        "                print(session.run(R))\n",
        "\n",
        "            if step % 100 == 0:\n",
        "                print(str(step) + \" minibatches completed\")\n",
        "\n",
        "            step += 1\n",
        "      \n",
        "        print(\"Trained decoder on \" + str(step) + \" minibatches.\\n\")\n",
        "    \n",
        "    # testing phase\n",
        "    print(\"***********************\")\n",
        "    print(\"Testing decoder...\")\n",
        "    print(\"***********************\")\n",
        "    for SNR in SNRs:\n",
        "      # simulate this SNR\n",
        "        sigma = np.sqrt(1. / (2 * (np.float(k)/np.float(n)) * 10**(SNR/10)))\n",
        "        frame_count = 0\n",
        "        bit_errors = 0\n",
        "        frame_errors = 0\n",
        "        frame_errors_with_HDD = 0\n",
        "        symbol_errors = 0\n",
        "        FE = 0\n",
        "\n",
        "      # simulate frames\n",
        "        while ((FE < min_frame_errors) or (frame_count < 100000)) and (frame_count < max_frames):\n",
        "            frame_count += batch_size # use different batch size for test phase?\n",
        "\n",
        "            if not ALL_ZEROS_CODEWORD_TESTING:\n",
        "            # generate message\n",
        "                messages = np.random.randint(0,2,[batch_size,k])\n",
        "\n",
        "            # encode message\n",
        "                codewords = np.dot(G, messages.transpose()) % 2\n",
        "\n",
        "            # modulate codeword\n",
        "                BPSK_codewords = (0.5 - codewords.astype(np.float32)) * 2.0\n",
        "\n",
        "         # add Gaussian noise to codeword\n",
        "            noise = sigma * np.random.randn(BPSK_codewords.shape[0],BPSK_codewords.shape[1])\n",
        "            channel_information = BPSK_codewords + noise\n",
        "\n",
        "         # convert channel information to LLR format\n",
        "            if NO_SIGMA_SCALING_TEST:\n",
        "                soft_input = channel_information\n",
        "            else:\n",
        "                soft_input = 2.0*channel_information/(sigma*sigma)\n",
        "\n",
        "         # run belief propagation\n",
        "            batch_data = soft_input\n",
        "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : codewords}\n",
        "            soft_outputs = session.run([belief_propagation], feed_dict=feed_dict)\n",
        "            soft_output = np.array(soft_outputs[0][1])\n",
        "            recovered_codewords = (soft_output < 0).astype(int)\n",
        "\n",
        "         # update bit error count and frame error count\n",
        "            errors = codewords != recovered_codewords\n",
        "            bit_errors += errors.sum()\n",
        "            frame_errors += (errors.sum(0) > 0).sum()\n",
        "\n",
        "            FE = frame_errors\n",
        "\n",
        "      # summarize this SNR:\n",
        "        print(\"SNR: \" + str(SNR))\n",
        "        print(\"frame count: \" + str(frame_count))\n",
        "\n",
        "        bit_count = frame_count * n\n",
        "        BER = float(bit_errors) / float(bit_count)\n",
        "        BERs.append(BER)\n",
        "        print(\"bit errors: \" + str(bit_errors))\n",
        "        print(\"BER: \" + str(BER))\n",
        "\n",
        "        FER = float(frame_errors) / float(frame_count)\n",
        "        FERs.append(FER)\n",
        "        print(\"FER: \" + str(FER))\n",
        "        print(\"\")\n",
        "\n",
        "    # print summary\n",
        "    print(\"BERs:\")\n",
        "    print(BERs)\n",
        "    print(\"FERs:\")\n",
        "    print(FERs)   \n",
        "\n",
        "    # offset = session.run(decoder.B_cv)\n",
        "    # weights = session.run(decoder.W_cv)"
      ]
    }
  ]
}